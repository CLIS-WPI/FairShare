services:
  fairness-dev:
    image: leo-fuzzy-fairness:dev
    container_name: fairness-dev
    runtime: nvidia  # CRITICAL: Required for GPU access
    build:
      context: ..
      dockerfile: docker/Dockerfile.dev
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Docker runtime level - makes GPUs visible to container
      - CUDA_VISIBLE_DEVICES=0,1  # TensorFlow level - explicit GPU IDs (not "all")
      - TF_FORCE_GPU_ALLOW_GROWTH=true  # Avoid OOM by growing memory as needed
      - TF_XLA_FLAGS=--tf_xla_enable_xla_devices --tf_xla_auto_jit=2  # Performance boost
      - PYTHONPATH=/workspace
      - JUPYTER_ENABLE_LAB=yes
    volumes:
      # Source code - live editing
      - ../src:/workspace/src
      - ../experiments:/workspace/experiments
      - ../data:/workspace/data
      - ../results:/workspace/results
      - ../notebooks:/workspace/notebooks
      - ../tests:/workspace/tests
      - ../scripts:/workspace/scripts
      # Root files (for scripts like verify_framework.py)
      - ../verify_framework.py:/workspace/verify_framework.py
      # Jupyter notebook data
      - jupyter-data:/root/.jupyter
      # Python cache (optional, for faster restarts)
      - python-cache:/root/.cache/pip
    ports:
      # Jupyter Lab
      - "8888:8888"
      # Jupyter Notebook (alternative)
      - "8889:8889"
      # TensorBoard (if needed)
      - "6006:6006"
    stdin_open: true
    tty: true
    shm_size: '2gb'  # Increased shared memory for TensorFlow
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Override entrypoint and keep container running with bash
    entrypoint: ["/bin/bash"]
    command: ["-c", "tail -f /dev/null"]

volumes:
  jupyter-data:
  python-cache:

